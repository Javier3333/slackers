[
    {
        "type": "message",
        "user": "U4K0WAADP",
        "text": "Let’s do 21st then — I’m free from 8:00 -- 10:30 or from noon — 2:00 - can you guys pick a time in there and send out an invite?",
        "ts": "1497368768.825633"
    },
    {
        "type": "message",
        "user": "U4K0WAADP",
        "text": "Also, do you guys go to SIGGRAPH?",
        "ts": "1497368811.844277"
    },
    {
        "type": "message",
        "user": "U13GPDA1F",
        "text": "Thanks, Ryan! Invite sent for 6\/21 - looking forward to it!",
        "ts": "1497371052.758562"
    },
    {
        "type": "message",
        "user": "U026PHQ3J",
        "text": "No one doing SIGGRAPH this year and tbh, i havent been since grad school cause twitter",
        "ts": "1497371107.780316"
    },
    {
        "type": "message",
        "user": "U4K0WAADP",
        "text": "I’m not sure what this is exactly, but I’m certain that it’s related to what we’re thinking about:\n\n<https:\/\/twitter.com\/soumithchintala\/status\/873206335508271106>",
        "attachments": [
            {
                "fallback": "<https:\/\/twitter.com\/soumithchintala|@soumithchintala>: Alyosha Efros showing a fun failure case of GANs in Moscow at the Machines Can See workshop. got a clap from the audience :) <https:\/\/pbs.twimg.com\/media\/DB5Adm7WsAQOG-b.jpg>",
                "ts": 1497023588,
                "author_name": "Soumith Chintala",
                "author_link": "https:\/\/twitter.com\/soumithchintala\/status\/873206335508271106",
                "author_icon": "https:\/\/pbs.twimg.com\/profile_images\/715921315975340033\/mAJ8CaZx_normal.jpg",
                "author_subname": "@soumithchintala",
                "text": "Alyosha Efros showing a fun failure case of GANs in Moscow at the Machines Can See workshop. got a clap from the audience :) <https:\/\/pbs.twimg.com\/media\/DB5Adm7WsAQOG-b.jpg>",
                "service_name": "twitter",
                "service_url": "https:\/\/twitter.com\/",
                "from_url": "https:\/\/twitter.com\/soumithchintala\/status\/873206335508271106",
                "image_url": "https:\/\/pbs.twimg.com\/media\/DB5Adm7WsAQOG-b.jpg",
                "image_width": 900,
                "image_height": 1200,
                "image_bytes": 184436,
                "id": 1,
                "footer": "Twitter",
                "footer_icon": "https:\/\/a.slack-edge.com\/6e067\/img\/services\/twitter_pixel_snapped_32.png"
            }
        ],
        "ts": "1497383965.593052"
    },
    {
        "type": "message",
        "user": "U4K0WAADP",
        "text": "Found it.\n\n<http:\/\/junyanz.github.io\/CycleGAN\/>",
        "attachments": [
            {
                "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
                "title_link": "http:\/\/junyanz.github.io\/CycleGAN\/",
                "fallback": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
                "image_url": "https:\/\/junyanz.github.io\/CycleGAN\/images\/teaser_fb.jpg",
                "from_url": "http:\/\/junyanz.github.io\/CycleGAN\/",
                "image_width": 478,
                "image_height": 250,
                "image_bytes": 234934,
                "service_name": "junyanz.github.io",
                "id": 1
            }
        ],
        "ts": "1497383995.603758"
    },
    {
        "type": "message",
        "user": "U026PHQ3J",
        "text": "so i’ve been reading a lot about these newer methods in GAN optimizers. Basically no one yet knows a best practice on the loss function, so everyone is still hammering away at it and doing this kind of “wtf” stuff",
        "ts": "1497385113.998534"
    },
    {
        "type": "message",
        "user": "U4K0WAADP",
        "text": "And here’s a new one that is getting a lot of buzz today — it’s mostly above my comprehension-level but I think it’s a potentially groundbreaking alternative to RNN’s &amp; CNN’s, and relies on just “attention”, whatever that means. My take is that it’s like Adderall for machine learning maybe?\n\n<https:\/\/arxiv.org\/abs\/1706.03762>",
        "ts": "1497399609.440137"
    },
    {
        "type": "message",
        "user": "U026PHQ3J",
        "text": "wow. hot off the presses",
        "ts": "1497409645.714266"
    },
    {
        "type": "message",
        "user": "U026PHQ3J",
        "text": "yeah this is a modification of of this",
        "ts": "1497409708.721289"
    },
    {
        "type": "message",
        "user": "U026PHQ3J",
        "text": "<https:\/\/arxiv.org\/abs\/1412.7755>",
        "ts": "1497409708.721339"
    },
    {
        "type": "message",
        "user": "U026PHQ3J",
        "text": "i’ve been doing a lot of work in RNN\/LSTM’s and these attention models use the recurrence architecture in convolutions.",
        "ts": "1497409754.726367"
    },
    {
        "type": "message",
        "user": "U026PHQ3J",
        "text": "reading this new paper (also: stoked)",
        "ts": "1497409768.727985"
    }
]